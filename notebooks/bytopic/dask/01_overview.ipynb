{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dask for Scalable Computing in Python\n",
    "\n",
    "<img src=\"../../../assets/dask_horizontal.svg\" align=\"right\" width=\"30%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Dask-for-Scalable-Computing-in-Python\" data-toc-modified-id=\"Dask-for-Scalable-Computing-in-Python-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Dask for Scalable Computing in Python</a></span><ul class=\"toc-item\"><li><span><a href=\"#Learning-Objectives\" data-toc-modified-id=\"Learning-Objectives-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Learning Objectives</a></span></li><li><span><a href=\"#What-is-&quot;Big-Data&quot;?\" data-toc-modified-id=\"What-is-&quot;Big-Data&quot;?-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>What is \"Big Data\"?</a></span></li><li><span><a href=\"#What-is-Dask?\" data-toc-modified-id=\"What-is-Dask?-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>What is Dask?</a></span><ul class=\"toc-item\"><li><span><a href=\"#Dask-Components\" data-toc-modified-id=\"Dask-Components-1.3.1\"><span class=\"toc-item-num\">1.3.1&nbsp;&nbsp;</span>Dask Components</a></span></li></ul></li><li><span><a href=\"#Going-Futher\" data-toc-modified-id=\"Going-Futher-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>Going Futher</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "\n",
    "- Define what we mean by \"Big Data\"\n",
    "- Get an overview of dask and its components "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## What is \"Big Data\"?\n",
    "\n",
    "\n",
    "There is a lot of hype around the buzzword \"big data\" today.  To avoid the ill-defined and often-overused term \"big data\", we’ll use a three-tiered definition throughout this tutorial to describe different-sized datasets. \n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "The boundaries between the following thresholds are a bit fuzzy and depend on how powerful your computer is. <b>The significane lies more in the different orders of magnitude rather than hard size limits</b>.\n",
    "</div>\n",
    "\n",
    "\n",
    "\n",
    "**An _opinioned_ tiered definition of data set sizes**\n",
    "\n",
    "| Dataset Type | Size range | Fits in RAM? | Fits on local disk? |\n",
    "| -------- | -------- | -------- |-----------------------|\n",
    "| Small dataset     | < 2-4 GB    | ✔    | ✔\n",
    "| Medium dataset     | < 2 TB    | ✖    | ✔\n",
    "| Large dataset     | > 2 TB   | ✖    | ✖\n",
    "\n",
    "\n",
    "- Small datasets:\n",
    "    - Fit comfortably in RAM, leaving memory to spare for manipulation and transformations.\n",
    "    - Usually no more than 2-4 GB in size.\n",
    "    - Complex opertions like aggregation can be done on these datasets without paging (spilling intermediate results to disk).\n",
    "    - Tools like NumPy, Pandas, Xarray, Scikit-learn are the best tools for the job. \n",
    "    - Throwing more sophisticated tools at these datasets is not only **overkill**, but can be counterproductive by adding unnecessary layers of complexity and overhead that can negatively impact performance. \n",
    "\n",
    "- Medium datasets:\n",
    "    - Cannot be held entirely in RAM but can fit comfortably in a single computer's local disk. \n",
    "    - Typically range in size from 10 GB to 2 TB.\n",
    "    - The same toolset used to analyze small datasets can be used to analyze medium datasets:\n",
    "      * A significant performance penality is imposed because these tools must use paging in order to avoid out of memory errors.\n",
    "    - Are large enough that it make senses to introduce parallelism (multithreading, multiprocessing) to cut down processing time.\n",
    "\n",
    "- Large datasets:\n",
    "    - Can neither fit in RAM nor fit in a single computer's persistent storage.\n",
    "    - Typically above 2 TB in size, can reach into petabytes and beyond.\n",
    "        - Once you have many TB of data to analyze, you are definitely in the realm of \"big data\"\n",
    "        - By this definition, many datasets we regularly confront in Earth science are in this category.\n",
    "    - NumPy, Pandas, Xarray alone are not suitable for datasets of this size, because they were not inherently built to operate on distributed datasets.\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Dask?\n",
    "\n",
    "- Dask is a tool that helps us easily extend our familiar Python data analysis toolset (NumPy, Pandas, Xarray, Scikit-learn, etc...) to medium and big data, i.e. dataset that can't fit in our computer's RAM. \n",
    "- In many cases, dask also allows us to speed up our analysis by using mutiple CPU cores. \n",
    "- Dask can help us work more efficiently on our laptop, and it can also help us scale up our analysis to thousand-node clusters on HPC and cloud platforms. \n",
    "- **Most importantly, dask is almost invisible to the user, meaning that you can focus on your science, rather than the details of parallel computing.**\n",
    "\n",
    "\n",
    "### Dask Components \n",
    "\n",
    "Dask consists of several different components and APIs, which can be categorized into three layers: \n",
    "\n",
    "- The scheduler\n",
    "- Low-level APIs\n",
    "- High-level APIs\n",
    "\n",
    "\n",
    "<img src=\"../../../assets/dask-components.jpeg\" align=\"right\" width=\"100%\">\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "It is probably easiest to illustrate what these mean through examples, so in the next notebook, we will jump right in. \n",
    "\n",
    "</div>\n",
    "\n",
    "## Going Futher\n",
    "\n",
    "- [The Dask Documentation](http://dask.readthedocs.io/en/latest/)\n",
    "- [The Dask Github Site](https://github.com/dask/dask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "  <p>Next: <a href=\"02_dask_arrays.ipynb\">Dask Arrays</a></p>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:python-tutorial]",
   "language": "python",
   "name": "conda-env-python-tutorial-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
