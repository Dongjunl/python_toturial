distributed:
  dashboard:
    link: http://{host}/proxy/{port}/status
  scheduler:
    bandwidth: 1000000000     # GB MB/s estimated worker-worker bandwidth
  worker:
    memory:
      target: 0.90  # Avoid spilling to disk
      spill: False  # Avoid spilling to disk
      pause: 0.80  # fraction at which we pause worker threads
      terminate: 0.95  # fraction at which we terminate the worker
  comm:
    compression: null

jobqueue:
  pbs:
    project: NCGD0011
    name: dask-worker
    cores: 36                   # Total number of cores per job
    memory: '109GB'             # Total amount of memory per job
    processes: 9                # Number of Python processes per job
    interface: ib0              # Network interface to use like eth0 or ib0
    queue: regular
    walltime: '04:00:00'
    resource-spec: select=1:ncpus=36:mem=109GB
    local-directory: "/glade/scratch/$USER/dask-tmp"

  slurm:
    project: NCGD0011
    name: dask-worker
    cores: 1                    # Total number of cores per job
    memory: '25GB'              # Total amount of memory per job
    processes: 1                # Number of Python processes per job
    interface: ib0
    walltime: '06:00:00'
    job-extra: {-C casper}
#    local-directory: "/glade/scratch/$USER/dask-tmp"
